{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Imports"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from typing import Literal\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import tqdm\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import AutoImageProcessor, AutoModelForImageClassification\n",
    "\n",
    "import data\n",
    "import lib\n",
    "\n",
    "from lib import predict_siglip"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Model instantiation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "restore_checkpoint: bool = False\n",
    "\n",
    "model_id = \"google/siglip2-large-patch16-384\"  # FixRes вариант\n",
    "model_preprocessor = AutoImageProcessor.from_pretrained(model_id)  # даст resize/normalize, mean/std/size\n",
    "\n",
    "optimizer = None\n",
    "\n",
    "# upcoming training epoch\n",
    "epoch = 0\n",
    "\n",
    "if restore_checkpoint:\n",
    "    epochs = lib.model_checkpoints(f'./models_siglip2/checkpoint_*.pth')\n",
    "\n",
    "    if len(epochs) == 0:\n",
    "        print('no models found')\n",
    "        raise ValueError('No model found')\n",
    "\n",
    "    print(f'Loading model from epoch { epochs[ 0 ] }')\n",
    "\n",
    "    checkpoint = torch.load(f'./models_siglip2/checkpoint_{ epochs[ 0 ] }.pth', weights_only=False)\n",
    "\n",
    "    model = checkpoint['model']\n",
    "    optimizer = checkpoint['optimizer']\n",
    "\n",
    "    epoch = model.epoch + 1\n",
    "else:\n",
    "    # Веса энкодера + НОВАЯ голова классификации (num_labels=2):\n",
    "    model = AutoModelForImageClassification.from_pretrained(\n",
    "        model_id,\n",
    "        num_labels=len(data.species_labels),\n",
    "        ignore_mismatched_sizes=True,  # создаст новую голову нужного размера\n",
    "    )\n",
    "\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    model.to(device)\n",
    "\n",
    "    model.tracking_loss = []\n",
    "    model.tracking_loss_val = []\n",
    "    model.tracking_accuracy = []\n",
    "    model.tracking_val_probs = []\n",
    "    # the last epoch we finished training on\n",
    "    model.epoch = None\n",
    "\n",
    "tracking_loss = model.tracking_loss\n",
    "tracking_loss_val = model.tracking_loss_val\n",
    "tracking_accuracy = model.tracking_accuracy\n",
    "tracking_val_probs = model.tracking_val_probs"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Data"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "train_ds = lib.ImageDatasetSigLip2(data.x_train, data.y_train, processor=model_preprocessor, learning=True)\n",
    "val_ds   = lib.ImageDatasetSigLip2(data.x_eval, data.y_eval, processor=model_preprocessor, learning=False)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=384, shuffle=True, num_workers=6)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=384, shuffle=False, num_workers=6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Freezing"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unfreezing: Literal['classifier_only', 'classifier_and_encoder', 'all'] = 'classifier_only'\n",
    "\n",
    "# C) Параметрические группы с «ступенчатым» LR: у головы LR выше, у энкодера ниже\n",
    "head_params = []\n",
    "enc_params  = []\n",
    "\n",
    "if unfreezing == 'classifier_only':\n",
    "    # 2) Заморозим всё, кроме головы (линейный пробинг)\n",
    "    for name, p in model.named_parameters():\n",
    "        p.requires_grad = \"classifier\" in name  # у HF-классификаторов голова обычно называется \"classifier\"\n",
    "\n",
    "        if \"classifier\" in name:\n",
    "            head_params.append(p)\n",
    "\n",
    "elif unfreezing == 'classifier_and_encoder':\n",
    "    # A) Сначала всё заморозим\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = False\n",
    "    for name, p in model.named_parameters():\n",
    "        if \"classifier\" in name:\n",
    "            p.requires_grad = True  # голова остаётся обучаемой\n",
    "            head_params.append(p)\n",
    "\n",
    "    # B) Разморозим последние L блоков визуального энкодера\n",
    "    L = 4  # начните с 2–4; при достаточном VRAM можно 6–8\n",
    "    layers = model.vision_model.encoder.layers   # ModuleList\n",
    "    for block in layers[-L:]:\n",
    "        for p in block.parameters():\n",
    "            p.requires_grad = True\n",
    "            enc_params.append(p)\n",
    "elif unfreezing == 'all':\n",
    "    for p in model.parameters():\n",
    "        p.requires_grad = True\n",
    "else:\n",
    "    raise ValueError(f\"Unknown unfreezing mode: {unfreezing}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loss (possibly with weights)"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 4) Баланс классов (простой вариант: веса в CrossEntropy по частотам)\n",
    "# import numpy as np\n",
    "# counts = np.bincount(train_labels, minlength=2)  # counts[0], counts[1]\n",
    "# class_weights = torch.tensor((counts.sum() / (2.0 * np.maximum(counts, 1))), dtype=torch.float32, device=device)\n",
    "# criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss(label_smoothing=0.1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Optimizer"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if optimizer is None:\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        [\n",
    "            {\"params\": enc_params,  \"lr\": 1e-4, \"weight_decay\": 0.05},\n",
    "            {\"params\": head_params, \"lr\": 1e-3, \"weight_decay\": 0.01},\n",
    "        ]\n",
    "    )"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Cutmix + mixup"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from torchvision.transforms import v2\n",
    "\n",
    "use_cutmix_mixup = True\n",
    "\n",
    "cutmix = v2.CutMix(num_classes=len(data.species_labels))\n",
    "mixup = v2.MixUp(num_classes=len(data.species_labels))\n",
    "cutmix_or_mixup = v2.RandomChoice([cutmix, mixup])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Loop"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_epochs = 1\n",
    "\n",
    "for cur_epoch in range(epoch, epoch + num_epochs):\n",
    "    print(f\"Starting epoch {cur_epoch}\")\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    loss_acc = 0\n",
    "    count = 0\n",
    "\n",
    "    for batch in tqdm.tqdm(train_loader, total=len(train_loader), desc='Training'):\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        images, labels = batch[\"pixel_values\"].to(torch.device(\"cuda\")), batch[\"labels\"].to(torch.device(\"cuda\"))\n",
    "\n",
    "        if use_cutmix_mixup:\n",
    "            images, labels = cutmix_or_mixup(images, labels)\n",
    "\n",
    "        out = model(images)              # logits: (B, 2)\n",
    "        loss = criterion(out.logits, labels)\n",
    "\n",
    "        c = batch['pixel_values'].size(0)\n",
    "        loss_acc += loss.item() * c\n",
    "        count += c\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    tracking_loss.append(loss_acc / count)\n",
    "\n",
    "    # валидация\n",
    "    model.eval()\n",
    "\n",
    "    probs, loss_acc = predict_siglip(\n",
    "        model, val_loader, accumulate_probs=True, accumulate_loss=True, desc='Validation', columns=data.species_labels, criterion=criterion\n",
    "    )\n",
    "    tracking_val_probs.append(probs)\n",
    "    tracking_loss_val.append(loss_acc)\n",
    "\n",
    "    eval_predictions = probs.idxmax(axis=1)\n",
    "    eval_true = data.y_eval.idxmax(axis=1)\n",
    "    correct = (eval_predictions == eval_true).sum()\n",
    "    accuracy = correct / len(eval_predictions)\n",
    "    tracking_accuracy.append(accuracy.item())\n",
    "\n",
    "    model.epoch = cur_epoch\n",
    "    lib.save_model(model, optimizer, f\"./models_siglip2/checkpoint_{str(cur_epoch).rjust(2, \"0\")}.pth\")\n",
    "\n",
    "    epoch = cur_epoch + 1\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Training progress"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "tracking_loss, tracking_loss_val, tracking_accuracy",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "epochs_train = list(range(len(tracking_loss)))\n",
    "epochs_val = list(range(len(tracking_loss_val)))\n",
    "\n",
    "line1, = ax.plot(epochs_train, tracking_loss, label=\"Train loss\")\n",
    "line2, = ax.plot(epochs_val, tracking_loss_val, label=\"Validation loss\")\n",
    "\n",
    "ax.set_xlabel(\"Epoch (index)\")\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.legend(loc=\"best\", handles=[line1, line2])\n",
    "\n",
    "ax.set_xticks(epochs_train)\n",
    "\n",
    "ax.grid(True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "fig, ax = plt.subplots(figsize=(15, 5))\n",
    "\n",
    "epochs_accuracy = list(range(len(tracking_accuracy)))\n",
    "\n",
    "line1, = ax.plot(epochs_accuracy, tracking_accuracy, label=\"Accuracy\", color=\"red\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "\n",
    "ax.legend(loc=\"best\", handles=[line1])\n",
    "\n",
    "ax.set_xticks(epochs_train)\n",
    "\n",
    "ax.grid(True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Validation"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "eval_preds_df = tracking_val_probs[-1]",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"True labels (training):\")\n",
    "data.y_train.idxmax(axis=1).value_counts(normalize=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"Predicted labels (eval):\")\n",
    "eval_preds_df.idxmax(axis=1).value_counts(normalize=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "print(\"True labels (eval):\")\n",
    "data.y_eval.idxmax(axis=1).value_counts(normalize=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "eval_predictions = eval_preds_df.idxmax(axis=1)\n",
    "eval_true = data.y_eval.idxmax(axis=1)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "correct = (eval_predictions == eval_true).sum()\n",
    "accuracy = correct / len(eval_predictions)\n",
    "accuracy.item()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Confusion matrix"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "cm = ConfusionMatrixDisplay.from_predictions(\n",
    "    data.y_eval.idxmax(axis=1),\n",
    "    eval_preds_df.idxmax(axis=1),\n",
    "    ax=ax,\n",
    "    xticks_rotation=30,\n",
    "    colorbar=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Create submission"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_dataset = lib.ImageDatasetSigLip2(data.test_features, processor=model_preprocessor, learning=False)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=6)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "submission_df = predict_siglip(model, test_dataloader)",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "submission_format = pd.read_csv(\"data/submission_format.csv\", index_col=\"id\")\n",
    "\n",
    "assert all(submission_df.index == submission_format.index)\n",
    "assert all(submission_df.columns == submission_format.columns)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "submission_df.to_csv(\"submission.csv\")",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
